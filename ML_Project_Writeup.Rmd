---
ytitle: "ML Project Writeup"
author: "MM"
date: "August 22, 2015"
output: html_document
---

## Background

In this project, for the Coursera class on *Practical Machine Learning*, data is used from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of this project is to predict how well the participants perform the barbell lifts.  More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset). 


## Choosing, Building and Implementing a Model

### Prediction Outcome Variable

In the training set, the [**classe**] variable represents how well each participant performs the barbell lifts, and this is what we'll be predicting in testing set.  The [**classe**] variable is defined as:

- **A**: The correct execution of the exercise
- **B**: Throwing the elbows to the front
- **C**: Lifting the dumbbell only halfway
- **D**: Lowering the dumbbell only halfway
- **E**: Throwing the hips to the front

### Selection of Predictor Variables

We begin by splitting the dataset into training and testing sets.

```{r, cache=TRUE, results="hide"}
## Working directory must be set
## Load package dependencies
library(caret)
library(ggplot2)
library(randomForest)

## Load the dataset
data <- read.csv("pml-training.csv")

## Split the dataset into training and testing sets
inTrain <- createDataPartition(y=data$classe, p=.6, list=FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]
```

By examining the structure of the training set, we notice a high proportion of `NA` values in a high number of columns.  The prediction model will exclude columns that have greater than 90% of its values as `NA`.  We also remove variables that are not of a numeric class under the assumption that non-numeric variables would not provide much predictive value as it relates to accelerometer measurements.

```{r}
## Define function to measure proportion of NA values in a given column
colNA <- function(x) {sum(is.na(x))/length(x)}

## Subset the training set for columns with less than 90% NA values
trainsub <- training[ , sapply(training, colNA)<.9]

## Remove other metadata
trainsub <- trainsub[ , 8:93]

## Remove other non-numeric variables
trainsub <- trainsub[ , !grepl("kurtosis", names(trainsub)) &
                              !grepl("skewness", names(trainsub)) &
                              !grepl("min", names(trainsub)) &
                              !grepl("max", names(trainsub)) &
                              !grepl("amplitude", names(trainsub))
                      ]
```

### Training a Machine Learning Algorithm

Given the complexity of the remaining measurements, we've elected to apply a **Random Forest** model to determine variable importance automatically.  Due to processing time constraints, we've limited the number of trees to 100.  We will use **5-fold cross validation** when applying the algorithm. 

```{r, cache=TRUE}
## Train the model on the testing set
modFit <- train(classe~., 
                data=trainsub, 
                method="rf", 
                trControl=trainControl(method="cv", number=5), 
                ntree=100
                )

print(modFit)
```

### Testing the Accuracy of the Model

Next, we'll apply the model to the testing set and measure its accuracy.  We must be sure to apply the same manipulations to the testing set that we applied to the training set.

```{r}
## Subset the training set in the same way that was done with the training set
testsub <- testing[ , names(trainsub)]

## Remove the classe variable for prediction
testsub <- testsub[ , -53]

## Apply model to testing set
pred <- predict(modFit, newdata=testsub)

## Calculate accuracy
confusionMatrix(testing$classe, pred)

accuracy <- confusionMatrix(testing$classe, pred)$overall[1]
```

From the output above we are acheiving high accuracy: `r accuracy`. We see an **out of bag error rate** of 1.08%.

```{r}
modFit$finalModel
```


## Conclusion

Of the machine learnging algorithms were applied, the Random Forest model achieved best results with classifying how well dumbell lifts were performed based on measured accelerometer data.

## Appendix

Variable Importance in the Random Forest Model

```{r}
varImp(modFit)
```

Plot of the two most important variables above in the training set

```{r}
qplot(pitch_forearm, roll_belt, data=trainsub, color=classe)
```


